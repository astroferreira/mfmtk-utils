{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('/data/github/mfmtk-utils/')\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from mfmtkutils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "0. Initialization\n",
    "1. Photometric Params\n",
    "     * [Effective Radii Rn](#rnandn)\n",
    "     * [The 'Spiking' Problem](#spiking)\n",
    "     * [Effective Radii Part #2](#eraddi2)\n",
    "     * [Sérsic Index n](#sersicindex)\n",
    "2. Morphometric Params\n",
    "    * [Concentration $C_1$](#c1)\n",
    "    * [Concentration $C_2$](#c2)\n",
    "3. Appendixes    \n",
    "    * [Catalog Class](#appclass)\n",
    "    * [Plotting Utilities](#appB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = '/data/catalogs/FERENGI/SDSS/flux/'\n",
    "\n",
    "\n",
    "redshifts = ['0.02', '0.03', '0.04', '0.05', '0.06', '0.07', '0.08', '0.09', '0.10', '0.12', \n",
    "             '0.14', '0.16', '0.18', '0.20', '0.25', '0.30', '0.35', '0.40', '0.45']\n",
    "\n",
    "T_type = np.loadtxt('/data/datasets/EFIGI_PGC.txt', usecols=[0, 1], dtype=str).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce and generate a vector in which each elements is a catalog for a given parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flux_cats = []\n",
    "for z in redshifts:\n",
    "    cat = catalog(path=path+z+'.mfmtk')\n",
    "    flux_cats.append(cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now flux_cats has information from each catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Which parameters do we need? (in order)\n",
    "photo_params = ['Mo', 'RnFit2D', 'nFit2D', 'Rp']\n",
    "morpho_params = ['C1', 'C2', 'A1', 'A3', 'S1', 'S3',\n",
    "                 'G', 'H', 'M20', 'sigma_psi']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can reduce flux_cats to produce catalogs for each parameter, let's start with the photo ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "photo_catalogs = []\n",
    "for j, param in enumerate(photo_params):\n",
    "    flux = flux_cats[0].reduce(flux_cats, param).raw_catalog    \n",
    "    photo_catalogs.append(flux[1:])\n",
    "    \n",
    "galaxies = flux_cats[0].raw_catalog[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the same for the morpho ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "morpho_catalogs = []\n",
    "for j, param in enumerate(morpho_params):\n",
    "    flux = flux_cats[0].reduce(flux_cats, param).raw_catalog\n",
    "    morpho_catalogs.append(flux[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes a long time to run, but it's a single run for the entire time working under this NB. For example, let's work with the image size only to check our process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z = np.array(redshifts).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Photometric Params\n",
    "## Effective Radii $R_n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"rnandn\"></div>\n",
    "Now, let's work with the Effective Radii (RnFit2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Mo = photo_catalogs[0].T.astype(float)\n",
    "Rn = photo_catalogs[1].T.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finds which galaxy is spiral or elliptical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "E_indexes = T_type.T[np.where(T_type[1].astype(float) < -2)].T[0]\n",
    "S_indexes = T_type.T[np.where((T_type[1].astype(float) >= 1) & (T_type[1].astype(float) < 7))].T[0]\n",
    "\n",
    "\n",
    "lenticulars  = np.array([i for i, val in enumerate(galaxies) if val in set(S0_indexes)])\n",
    "spirals = np.array([i for i, val in enumerate(galaxies) if val in set(S_indexes)])\n",
    "ellipticals = np.array([i for i, val in enumerate(galaxies) if val in set(E_indexes)])\n",
    "print '# Spirals:', spirals.shape[0]\n",
    "print '# Ellipticals:', ellipticals.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"spiking\"></div>\n",
    "# Spiking Effect (Sérsic 2D fit errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the overall behavior of $R_n$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(z, Rn.T, '-k', alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot reveals some degree of error in our measurements. There is a spiking effect in some redshift slices, tracking those galaxies would be useful. We could do that by taking the derivative of our measurements and finding where there is a increase in the function since it is overall monotonic decreasing. Sure we would lose some information about ''natural'' increasing not due measurement erros, but in a whole this method works. We take the derivative for each galaxy, np.gradient would take the spatial derivative for all the 2D $R_n$ matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spiked = []\n",
    "not_spiked = []\n",
    "for i, galaxy in enumerate(Rn):\n",
    "    derivative = np.gradient(galaxy)\n",
    "    if not (np.size(derivative[np.where(derivative > 0)]) > 0):\n",
    "        not_spiked.append(i)\n",
    "    else:\n",
    "        spiked.append(i)\n",
    "spiked = np.array(spiked)\n",
    "not_spiked = np.array(not_spiked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and just for the sake of simplicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spirals_not_spiked = intersect(spirals, not_spiked)\n",
    "ellipticals_not_spiked = intersect(ellipticals, not_spiked)\n",
    "print not_spiked.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
    "ax1.plot(z, Rn[spiked].T, '-k')\n",
    "ax2.plot(z, Rn[not_spiked].T, '-k');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our objective is not to exclude these galaxies but to track them down and verify morfometryka measurement on them. galaxies[spiked] give us every one of them but we would want only worst cases for it is a lot more easier to find the problem with tem. We increased the threshold for the number of derivatives to 5, so only galaxies showing 5 spikes are allowed in our selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reduced our pool of galaxies from $\\sim 500$ to $16$. Let's write them in a catalog for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('/data/worst_spiking_cases.mfmtk', 'w')\n",
    "for gal in galaxies[spiked]:\n",
    "    f.write(gal + '\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"eraddi2\"></div>\n",
    "# Effective Radii Part #2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to extract the resolution behavior by multiplying our Rn measurement by the scale factor $n$ that \n",
    "$$ S_{f} = S_{i} n $$\n",
    "where S stands for 'Size'. The image is a square, width and height are the same. To find the right Rn we need to\n",
    "$$ Rn = \\frac{R_z}{n} $$\n",
    "The initial image size is 1024, so\n",
    "$$ n = \\frac{Mo}{1024}  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Rn_f = 1024 * Rn / Mo\n",
    "f, axes = plt.subplots(4, 5, figsize=(15, 7))\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "histograms(Rn_f[spirals], z, axes)\n",
    "histograms(Rn_f[ellipticals], z, axes, color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph shows some spikes due to errors of measurement by Morfometryka (on Sérsic profile fits). The problem is that some galaxies becomes very faint in these redshift simulations and we're not able to performe realiable measurements, but working with a big sample diminish the effect produced by outliers. What if we make a linear regression for each galaxy and then see how the distribution of regressions behave? Let's do that for all galaxies first, then we could use the indexes in ''spirals'' and ''ellipticals'' to retrive them and treat them separately. So, we have something like this\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Rn_fits = []\n",
    "for galaxy in Rn_f:\n",
    "    fit = np.polyfit(z, galaxy, 1)\n",
    "    Rn_fits.append(fit)\n",
    "\n",
    "Rn_fits = np.array(Rn_fits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here it's trickier to plot all the fits in the same way we did before, let's plot them separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for fit in Rn_fits[spirals]:\n",
    "    plt.plot(z, z*fit[0] + fit[1], '-b')\n",
    "    \n",
    "for fit in Rn_fits[ellipticals]:    \n",
    "    plt.plot(z, z*fit[0] + fit[1], '-r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overwall, the spiking effect disappeared and the distribution in the bottom seems strong enough to give us some insights, let's see how the mean for each class behaves. Here, we use the ''axis'' argument from np.mean() to take the mean from each column of the data (each redshift step), the same is valid for the standard deviation. So each element of the new matrix is a tuple (mean, std) for each redshift step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "RnS_stats = (Rn_f[spirals].T.mean(axis=1), Rn_f[spirals].T.std(axis=1))\n",
    "RnE_stats = (Rn_f[ellipticals].T.mean(axis=1), Rn_f[ellipticals].T.std(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need statstics from our fits too. Let's plot only the classes distributions information, we'll use a little bit of jittering to distinguish the error bars (out standard deviation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.errorbar(z, RnS_stats[0], yerr=RnS_stats[1])\n",
    "plt.errorbar(z + 0.005, RnE_stats[0], yerr=RnE_stats[1], color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sérsic Index $n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"sersicindex\"></div>\n",
    "This validates to us that within some scattering, Morfometryka is able to retrieve the information about structural parameters (size, in this case) even in low SNR and resolution regimes. Now, we can apply this procedure to each of the parameters. Let's proceed to the Sérsic Index here called nFit2D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = photo_catalogs[2].T.astype(float)\n",
    "f, axes = plt.subplots(4, 5, figsize=(15, 7))\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "histograms(n[spirals_not_spiked], z, axes)\n",
    "histograms(n[ellipticals_not_spiked], z, axes, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_fits = []\n",
    "for galaxy in n:\n",
    "    fit = np.polyfit(z, galaxy, 1)\n",
    "    n_fits.append(fit)\n",
    "\n",
    "n_fits = np.array(n_fits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for fit in n_fits[spirals]:\n",
    "    plt.plot(z, z*fit[0] + fit[1], '-b')\n",
    "    \n",
    "for fit in n_fits[ellipticals]:    \n",
    "    plt.plot(z, z*fit[0] + fit[1], '-r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nS_stats = (n[spirals].T.mean(axis=1), n[spirals].T.std(axis=1))\n",
    "nE_stats = (n[ellipticals].T.mean(axis=1), n[ellipticals].T.std(axis=1))\n",
    "plt.errorbar(z, nS_stats[0], yerr=nS_stats[1])\n",
    "plt.errorbar(z + 0.005, nE_stats[0], yerr=nE_stats[1], color='red')\n",
    "plt.xlim([0.02, 0.50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Morphometric Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concentrations $C_1$ and $C_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ C_1 = \\log_{10} \\left ( \\frac{R_{80}}{R_{20}} \\right ) $$ $$ C_2 = \\log_{10} \\left ( \\frac{R_{90}}{R_{50}} \\right ) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "C1 = morpho_catalogs[0].astype(float).T/5\n",
    "C2 = morpho_catalogs[1].astype(float).T/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from mfmtkutils import *\n",
    "f, (ax1, ax2) = plt.subplots(1,2, sharey=True, sharex=True, figsize=(12, 5))\n",
    "plt.subplots_adjust(wspace=0)\n",
    "ax1.set_xlim([0.02, 0.45])\n",
    "plot_as_gaussians(C1[spirals], z, ax1)\n",
    "plot_as_gaussians(C1[ellipticals], z, ax1, color='red')\n",
    "plot_as_gaussians(C2[spirals], z, ax2, label=r'$\\rm Spirals$')\n",
    "plot_as_gaussians(C2[ellipticals], z, ax2, color='red')\n",
    "\n",
    "plt.legend(loc=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $C_1$ distributions for each redshift are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(3, 7, figsize=(15, 5))\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "histograms(C1[spirals], z, axes, normed=1)\n",
    "histograms(C1[ellipticals], z, axes, color='red', normed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the same for $C_2$ gives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(3, 7, figsize=(15, 5))\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "histograms(C2[spirals], z, axes, normed=1)\n",
    "histograms(C2[ellipticals], z, axes, color='red', normed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting bot classes for $C_1$ give us"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Apparently it seems that both classes decreases linearly and the spiking is small here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "C1S_stats = (C1[spirals].T.mean(axis=1), C1[spirals].T.std(axis=1))\n",
    "C1E_stats = (C1[ellipticals].T.mean(axis=1), C1[ellipticals].T.std(axis=1))\n",
    "plt.errorbar(z, C1S_stats[0], yerr=C1S_stats[1])\n",
    "plt.errorbar(z + 0.005, C1E_stats[0], yerr=C1E_stats[1], color='red')\n",
    "plt.xlim([0.02, 0.25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make a more meaningful plot let's use every information we got. See [Plot Utilities](#appA) for more information about our plotting routines. Here we have two takes on density plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "C1_fits = bulk_linear_fit(C1).T\n",
    "plot_final(C1, C1S_stats, C1E_stats, C1_fits, spirals, ellipticals, ax=ax1)\n",
    "density_mesh_plot(z, C1[ellipticals],cmap=\"Oranges\", ax=ax2)\n",
    "density_mesh_plot(z, C1[spirals], ax=ax2, mask_threshold=10)\n",
    "ax1.set_xlim([0.02, 0.25])\n",
    "ax2.set_xlim([0.02, 0.25])\n",
    "ax1.set_ylim([2, 5])\n",
    "ax2.set_ylim([2, 5])\n",
    "ax2.set_yticks([])\n",
    "plt.subplots_adjust(wspace=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, both linear fits describes the overall distribution very well. There's a lot of scattering, but filled region shows $1 \\ \\sigma$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The one in the left shows the means for both classes distribution of measurements with solid lines and $\\pm  1 \\sigma$ in the shaded area. Right plot show a box mesh density distribution. We would need to performe all steps for each of the parameters, how convinient would it be to plot everything at the same time and then follow with treatment only in the special cases? Here it goes. We want 10 plots, one for each parameter. Let's make 2 columns with 5 rows each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(5, 2, figsize=(10, 18))\n",
    "labels = [r'$\\rm C_1$', r'$\\rm C_2$', r'$\\rm A_1$', r'$\\rm A_3$',\n",
    "          r'$\\rm S_1$', r'$\\rm S_3$', r'$\\rm G$', r'$\\rm H$', r'$\\rm M_20$', r'$\\rm \\sigma_\\psi $']\n",
    "morpho_params = ['C1', 'C2', 'A1', 'A3', 'S1', 'S3',\n",
    "                 'G', 'H', 'M20', 'sigma_psi']\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    param = morpho_catalogs[i].astype(float).T\n",
    "    paramS_stats = (param[spirals].T.mean(axis=1), param[spirals].T.std(axis=1))\n",
    "    paramE_stats = (param[ellipticals].T.mean(axis=1), param[ellipticals].T.std(axis=1))\n",
    "    #param_fits = bulk_linear_fit(param).T\n",
    "    plot_final(param, paramS_stats, paramE_stats, spirals, ellipticals, ax=ax)\n",
    "    ax.set_xlim([0.02, 0.50])\n",
    "    if(i < 8):\n",
    "        ax.set_xticks([])\n",
    "    else:\n",
    "        ax.set_xlabel(r'$\\rm Redshift$', fontsize=20)\n",
    "    ax.set_ylabel(labels[i], fontsize=20)\n",
    "    if not (i % 2 == 0):\n",
    "        ax.yaxis.tick_right()\n",
    "        ax.yaxis.set_label_position(\"right\")\n",
    "    \n",
    "plt.subplots_adjust(hspace=0, wspace=0)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, as we can see, only the last three plots got problems, we now know about that and can proceed to work in each at the time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could make a linear fit for each class data and use it's standard deviation as follows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"appclass\"> </div>\n",
    "# Appendix A: The Catalog class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"appB\"></div>\n",
    "# Appendix B: Plotting Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_final(C1, C1S_stats, C1E_stats, spirals, ellipticals, ax=False, points=False):\n",
    "    if not ax:\n",
    "        if(points):\n",
    "            #Plot Points with Jittering\n",
    "            for gal in C1[spirals]:\n",
    "                plt.plot(z+0.1*z*np.random.rand(), gal, 'xb', alpha=0.2)\n",
    "\n",
    "            for gal in C1[ellipticals]:\n",
    "                plt.plot(z+0.1*z*np.random.rand(), gal, 'vr', alpha=0.2)\n",
    "\n",
    "        #Plot means and fits\n",
    "        plt.plot(z, C1S_stats[0], '--k', lw=2)\n",
    "        plt.plot(z, C1E_stats[0], '--k')\n",
    "        \n",
    "\n",
    "\n",
    "        plt.fill_between(z, C1S_stats[0] - C1S_stats[1],\n",
    "                         C1S_stats[0] + C1S_stats[1], alpha=0.5)\n",
    "        \n",
    "        plt.fill_between(z, C1E_stats[0] - C1E_stats[1],\n",
    "                         C1E_stats[0] + C1E_stats[1], alpha=0.5, facecolor='red')\n",
    "        \n",
    "    else:\n",
    "        if(points):\n",
    "            for gal in C1[spirals]:\n",
    "                ax.plot(z+z*np.random.rand(), gal, 'xb', alpha=0.2)\n",
    "\n",
    "            for gal in C1[ellipticals]:\n",
    "                ax.plot(z+z*np.random.rand(), gal, 'vr', alpha=0.2)\n",
    "\n",
    "        #Plot means and fits\n",
    "        ax.plot(z, C1S_stats[0], '--k', lw=2)\n",
    "        ax.plot(z, C1E_stats[0], '--k')\n",
    "\n",
    "        \n",
    "        ax.fill_between(z, C1S_stats[0] - C1S_stats[1],\n",
    "                         C1S_stats[0] + C1S_stats[1], alpha=0.5)\n",
    "        \n",
    "        ax.fill_between(z, C1E_stats[0] - C1E_stats[1],\n",
    "                         C1E_stats[0] + C1E_stats[1], alpha=0.5, facecolor='red')\n",
    "        \n",
    "def bulk_linear_fit(param):\n",
    "    param_fits = []\n",
    "    for galaxy in param:\n",
    "        fit = np.polyfit(z, galaxy, 1)\n",
    "        param_fits.append(fit)\n",
    "    \n",
    "    return np.array(param_fits)\n",
    "\n",
    "def plot_linear_fit(galaxies, params='-b'):\n",
    "    for fit in galaxies:\n",
    "        plt.plot(z, z*fit[0] + fit[1], params)\n",
    "    \n",
    "    \n",
    "def parse_to_points(x, y):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for c1 in y:\n",
    "        for zi, ci in zip(z, c1):\n",
    "            xs.append(zi + 0.5*zi*np.random.rand())\n",
    "            ys.append(ci)\n",
    "    \n",
    "    return xs, ys\n",
    "\n",
    "\n",
    "def density_mesh_plot(x, y, cmap=\"Blues\", mask_threshold=1, ax=None):\n",
    "    y[np.where(np.isnan(y))] = 0\n",
    "    x_p, y_p = parse_to_points(x, y)\n",
    "    H, xedges, yedges = np.histogram2d(x_p, y_p,  bins=(60, 20))\n",
    "    H = np.rot90(H)\n",
    "    H = np.flipud(H)\n",
    "    H = np.ma.masked_array(H, H < mask_threshold)\n",
    "    if(ax):\n",
    "        ax.pcolormesh(xedges, yedges, H, cmap=cmap)\n",
    "    else:\n",
    "        plt.pcolormesh(xedges, yedges, H, cmap=cmap)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
